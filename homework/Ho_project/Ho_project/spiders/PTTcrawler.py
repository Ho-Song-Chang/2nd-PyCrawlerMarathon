# -*- coding: utf-8 -*-
import scrapy
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pprint import pprint

# 例目司W址: https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html
class PttcrawlerSpider(scrapy.Spider):
    name = 'PTTCrawler'
    allowed_domains = ['www.ptt.cc']
    start_urls = ['https://www.ptt.cc/bbs/Gossiping/M.1557928779.A.0C1.html']
    cookies = {'over18': '1'}

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url=url, callback=self.parse, cookies=self.cookies)

    def parse(self, response):
        # 假OW回不是 200 OK 的, 我魉驼求失
        if response.status != 200:
            print('Error - {} is not available to access'.format(response.url))
            return

        # ⒕W回的 HTML 魅 BeautifulSoup 解析器, 方便我根嘶` (tag) Y去^Vふ
        soup = BeautifulSoup(response.text)

        
        # 取得文章热葜黧w
        main_content = soup.find(id='main-content')
        
        # 假如文章有傩再Y料 (meta), 我在傩缘^K中爬出作者 (author), 文章祟} (title), l文日期 (date)
        metas = main_content.select('div.article-metaline')
        author = ''
        title = ''
        date = ''
        if metas:
            if metas[0].select('span.article-meta-value')[0]:
                author = metas[0].select('span.article-meta-value')[0].string
            if metas[1].select('span.article-meta-value')[0]:
                title = metas[1].select('span.article-meta-value')[0].string
            if metas[2].select('span.article-meta-value')[0]:
                date = metas[2].select('span.article-meta-value')[0].string

            #  main_content 中移除 meta Y（author, title, date c其他看板Y）
            #
            # .extract() 方法可以⒖脊俜轿募
            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract
            for m in metas:
                m.extract()
            for m in main_content.select('div.article-metaline-right'):
                m.extract()
        
        # 取得留言^主w
        pushes = main_content.find_all('div', class_='push')
        for p in pushes:
            p.extract()
        
        # 假如文章中有包含「※ l信站: 批踢踢I坊(ptt.cc), 碜: xxx.xxx.xxx.xxx」的邮
        # 透^ regular expression 取得 IP
        # 因樽执中包含特殊符跟中文, @建h使用 unicode 的型式 u'...'
        try:
            ip = main_content.find(text=re.compile(u'※ l信站:'))
            ip = re.search('[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*', ip).group()
        except Exception as e:
            ip = ''
        
        # 移除文章主w中 '※ l信站:', '◆ From:', 空行及多N空白 (※ = u'\u203b', ◆ = u'\u25c6')
        # 保留英底, 中文及中文它c, W址, 部分特殊符
        #
        # 透^ .stripped_strings 的方式可以快速移除多N空白K取出文字, 可⒖脊俜轿募 
        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings
        filtered = []
        for v in main_content.stripped_strings:
            # 假如字串_^不是特殊符或是以 '--' _^的, 我都保留其文字
            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:
                filtered.append(v)

        # 定x一些特殊符c全形符的^V器
        expr = re.compile(u'[^一-。；，：“”（）、？《》\s\w:/-_.?~%()]')
        for i in range(len(filtered)):
            filtered[i] = re.sub(expr, '', filtered[i])
        
        # 移除空白字串, M合^V後的文字即槲恼卤疚 (content)
        filtered = [i for i in filtered if i]
        content = ' '.join(filtered)
        
        # 理留言^
        # p 算推文盗
        # b 算u文盗
        # n 算箭^盗
        p, b, n = 0, 0, 0
        messages = []
        for push in pushes:
            # 假如留言段落]有 push-tag 就跳^
            if not push.find('span', 'push-tag'):
                continue
            
            # ^V~外空白cQ行符
            # push_tag 判嗍峭莆, 箭^是u文
            # push_userid 判嗔粞缘娜耸钦l
            # push_content 判嗔粞热
            # push_ipdatetime 判嗔粞匀掌rg
            push_tag = push.find('span', 'push-tag').string.strip(' \t\n\r')
            push_userid = push.find('span', 'push-userid').string.strip(' \t\n\r')
            push_content = push.find('span', 'push-content').strings
            push_content = ' '.join(push_content)[1:].strip(' \t\n\r')
            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \t\n\r')

            # 整理打包留言的Y, Ky推u文盗
            messages.append({
                'push_tag': push_tag,
                'push_userid': push_userid,
                'push_content': push_content,
                'push_ipdatetime': push_ipdatetime})
            if push_tag == u'推':
                p += 1
            elif push_tag == u'u':
                b += 1
            else:
                n += 1
        
        # y推u文
        # count 橥u文相抵看@篇文章推文是u文比^多
        # all 榭共留言盗 
        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}
        
        # 整理文章Y
        data = {
            'url': response.url,
            'article_author': author,
            'article_title': title,
            'article_date': date,
            'article_content': content,
            'ip': ip,
            'message_count': message_count,
            'messages': messages
        }
        yield data
